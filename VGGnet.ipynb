{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0emeGFn3P65B"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06awPuT1P3iU",
        "outputId": "dcfc9be4-cf15-45e6-bc68-916c87c2ec82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.preprocessing.image import load_img, smart_resize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import random\n",
        "import time\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons.layers\n",
        "from tensorflow_addons.layers import InstanceNormalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVKBHgXmQOsv"
      },
      "source": [
        "# paramètre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEaYfs8ZlbaY",
        "outputId": "74768bc9-ca51-4b63-fea7-495d22dc6831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3xgtqqsQS1M"
      },
      "outputs": [],
      "source": [
        "# General data parameters\n",
        "#OU/Projet_tri_dechets/dataset\n",
        "DATASET_PATH = '/content/drive/MyDrive/dataset'\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "# Data generator parameters\n",
        "TRAINING_BATCH_SIZE = 84 #64 au debut\n",
        "TRAINING_IMAGE_SIZE = (128, 128)  #au départ c'était 128\n",
        "VALIDATION_BATCH_SIZE = 50\n",
        "VALIDATION_IMAGE_SIZE = (128, 128)\n",
        "TESTING_BATCH_SIZE = 16   #au départ 16\n",
        "TESTING_IMAGE_SIZE = (128, 128)\n",
        "NUMBER_OF_CHANNELS = 3\n",
        "TRAIN_SIZE = 0.6\n",
        "VALIDATION_SIZE = 0.2\n",
        "PROBA_AUG=0.6\n",
        "\n",
        "#Model parameters\n",
        "KERNEL_SIZE=(2,2)\n",
        "DROPOUT_RATE=0.2  #0.35 par default\n",
        "\n",
        "#Training parameters\n",
        "NOMBRE_EPOCHS = 65"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qK4UwBVP_BG"
      },
      "source": [
        "# générateur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41-Fo5SlPrEC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_generators(data_path=DATASET_PATH):\n",
        "    'Returns three generators'\n",
        "    image_paths = []\n",
        "    for folder in os.listdir(data_path):\n",
        "        paths_to_add = [os.path.join(folder, path) for path in os.listdir(os.path.join(data_path, folder)) if path.endswith('jpg')]\n",
        "        image_paths = image_paths + paths_to_add\n",
        "\n",
        "    train_list, val_list, test_list = data_split(np.asarray(image_paths))\n",
        "\n",
        "    train_data_generator = DataGeneratorClassifier(train_list, TRAINING_BATCH_SIZE, TRAINING_IMAGE_SIZE)\n",
        "    validation_data_generator = DataGeneratorClassifier(val_list, VALIDATION_BATCH_SIZE, VALIDATION_IMAGE_SIZE)\n",
        "    test_data_generator = DataGeneratorClassifier(test_list, TESTING_BATCH_SIZE, TESTING_IMAGE_SIZE)\n",
        "    return train_data_generator, validation_data_generator, test_data_generator\n",
        "\n",
        "\n",
        "def data_split(paths_list):\n",
        "    'Splits the paths list into three splits'\n",
        "    split_1 = int(TRAIN_SIZE*len(paths_list))\n",
        "    split_2 = int((TRAIN_SIZE+VALIDATION_SIZE)*len(paths_list))\n",
        "    np.random.shuffle(paths_list)\n",
        "    return paths_list[:len(paths_list)-100], paths_list[len(paths_list)-100:len(paths_list)-50], paths_list[len(paths_list)-50:]\n",
        "\n",
        "\n",
        "class DataGeneratorClassifier(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, batch_size, image_size, data_path=DATASET_PATH, n_channels=NUMBER_OF_CHANNELS, shuffle=SHUFFLE_DATA):\n",
        "        'Initialisation'\n",
        "        self.classes = os.listdir(data_path)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.data_path=data_path\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = self.list_IDs[indexes]\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        \n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *image_size, n_channels)\n",
        "        X = np.empty((self.batch_size, *self.image_size, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            \n",
        "            Xi = load_img(os.path.join(self.data_path, ID))\n",
        "            Xi = smart_resize(np.asarray(Xi), self.image_size)\n",
        "      \n",
        "            # because as we alreay load image into the memory, so we are using flow() function, to apply transformation\n",
        "            X[i,:] = Xi\n",
        "            y[i] = self.classes.index(ID.split('/')[0])\n",
        "        data_augm=tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),layers.experimental.preprocessing.RandomRotation(0.0),layers.experimental.preprocessing.RandomContrast(0.01)])\n",
        "        X=data_augm(X)\n",
        "        return X,tf.keras.utils.to_categorical(y,num_classes=10)\n",
        "\n",
        "def show_batch(generator, batch_number=0):\n",
        "    images, labels = generator.__getitem__(batch_number)\n",
        "    width = int(np.floor(np.sqrt(labels.shape[0])))\n",
        "    height = int(np.ceil(labels.shape[0]/float(width)))\n",
        "    total_height = int(0.09*height*images.shape[1])\n",
        "    total_width = int(0.09*width*images.shape[2])\n",
        "    f, axarr = plt.subplots(height,width, figsize=(total_height,total_width))\n",
        "    for image in range(images.shape[0]):\n",
        "        image_to_show = (images[image])/np.max(images[image])\n",
        "        axarr[image//width,image%width].imshow(image_to_show)\n",
        "        axarr[image//width,image%width].set_title(generator.classes[np.argmax(labels[image])])\n",
        "    f.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNuWIaksQkGb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VJuVWHAQnUW"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(33)\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, KERNEL_SIZE, activation='gelu', padding='same', strides=1), \n",
        "    layers.Conv2D(32, KERNEL_SIZE, activation='gelu', padding='same', strides=1),\n",
        "    layers.MaxPool2D(pool_size=2),\n",
        "    layers.Conv2D(64, KERNEL_SIZE, activation='gelu', padding='same', strides=1),\n",
        "    layers.Conv2D(64, KERNEL_SIZE, activation='gelu', padding='same', strides=1),\n",
        "    layers.MaxPool2D(pool_size=2),\n",
        "    layers.Conv2D(128, KERNEL_SIZE, activation='gelu', padding='same', strides=1), \n",
        "    layers.Conv2D(128, KERNEL_SIZE, activation='gelu', padding='same', strides=1),\n",
        "    layers.MaxPool2D(pool_size=2),\n",
        "    layers.Conv2D(256, KERNEL_SIZE, activation='gelu', padding='same', strides=1), \n",
        "    layers.Conv2D(256, KERNEL_SIZE, activation='gelu', padding='same', strides=1),\n",
        "    layers.MaxPool2D(pool_size=2),\n",
        "    # layers.Conv2D(512, KERNEL_SIZE, activation='relu', padding='same', strides=1), \n",
        "    # layers.Conv2D(512, KERNEL_SIZE, activation='relu', padding='same', strides=1),\n",
        "    # layers.MaxPool2D(pool_size=2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(100, activation='gelu'),\n",
        "    layers.Dropout(rate=DROPOUT_RATE),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.build(input_shape=(TRAINING_BATCH_SIZE, *TRAINING_IMAGE_SIZE, NUMBER_OF_CHANNELS))\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#model.summary() \n",
        "\n",
        "## Sauvegarde du model\n",
        "from keras.models import load_model\n",
        "\n",
        "model.save('/content/drive/MyDrive/Projet_tri_dechets')  # creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "\n",
        "#model = load_model('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbZt4IM9Qtb0"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcnUYuQ7Qy-J"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  k=create_generators(data_path=DATASET_PATH)\n",
        "  training_generator=k[0]\n",
        "  validation_generator=k[1]\n",
        "  history=model.fit(training_generator,validation_data=validation_generator,epochs=NOMBRE_EPOCHS)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTdMQ6gcQ2Mc"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcgOBC45Q7Fa"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  test_generator = create_generators(DATASET_PATH)[2]\n",
        "  model.score(test_generator)\n",
        "  return('pas de test')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY1IQI2tRChJ"
      },
      "source": [
        "# Main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgRbYA8yBPqS"
      },
      "outputs": [],
      "source": [
        "\n",
        "#show_batch(test_generator,batch_number=1)\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}